{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11210601,"sourceType":"datasetVersion","datasetId":7000183}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install facenet_pytorch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing Neccessary Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport copy\nimport torch\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image, ImageFile\nfrom facenet_pytorch import MTCNN\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader,random_split\nfrom scipy.spatial.distance import pdist, squareform\nfrom torch.utils.data import Subset, DataLoader\nfrom torchvision import transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Allow loading of truncated images to avoid errors with corrupted files\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:02:57.759139Z","iopub.execute_input":"2025-03-30T21:02:57.759597Z","iopub.status.idle":"2025-03-30T21:02:57.767503Z","shell.execute_reply.started":"2025-03-30T21:02:57.759571Z","shell.execute_reply":"2025-03-30T21:02:57.766590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_pseudo_labels(image_dir, output_json):\n    \"\"\"\n    Generate pseudo-labels for head counting using MTCNN for face detection.\n    Detected bounding boxes are saved as pseudo-labels in a JSON file.\n    \n    Args:\n        image_dir (str): Directory containing the images.\n        output_json (str): Output JSON file path to save pseudo-labels.\n    \"\"\"\n    # Initialize the MTCNN model for detecting multiple faces per image.\n    mtcnn = MTCNN(keep_all=True)\n    pseudo_labels = {}\n\n    # Process each image in the specified directory.\n    for img_file in os.listdir(image_dir):\n        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n            img_path = os.path.join(image_dir, img_file)\n            try:\n                image = Image.open(img_path).convert('RGB')\n            except OSError as e:\n                print(f\"Error loading image {img_file}: {e}. Skipping this file.\")\n                continue\n\n            # Detect faces (as proxy for heads)\n            boxes, probs = mtcnn.detect(image)\n\n            # Convert detected boxes to integer coordinates if detections are present.\n            if boxes is not None:\n                boxes = boxes.tolist()\n                boxes = [[int(x) for x in box] for box in boxes]\n            else:\n                boxes = []\n\n            pseudo_labels[img_file] = boxes\n            print(f\"Processed {img_file} - Detected {len(boxes)} heads.\")\n    \n    # Save the pseudo-labels to a JSON file.\n    with open(output_json, 'w') as f:\n        json.dump(pseudo_labels, f, indent=4)\n    print(f\"Pseudo-labels saved to {output_json}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_detection(image_path, boxes):\n    \"\"\"\n    Visualize the detections on a single image.\n    \n    Args:\n        image_path (str): Path to the image.\n        boxes (list): List of bounding boxes [x1, y1, x2, y2].\n    \"\"\"\n    image = Image.open(image_path).convert('RGB')\n    fig, ax = plt.subplots(1)\n    ax.imshow(image)\n    for box in boxes:\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n    plt.title(\"Detected Heads\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Dataset path for the training data.\n    image_dir = \"/kaggle/input/crowd-data/Train Data\"\n    output_json = \"/kaggle/working/pseudo_labels.json\"\n    \n    # Generate pseudo-labels using weak supervision (pseudo labeling).\n    generate_pseudo_labels(image_dir, output_json)\n    \n    # visualize detection for one sample image:\n    with open(output_json, 'r') as f:\n        pseudo_labels = json.load(f)\n        for image_file, boxes in pseudo_labels.items():\n            image_path = os.path.join(image_dir, image_file)\n            visualize_detection(image_path, boxes)\n            break  # Visualize only the first image.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:03:20.611987Z","iopub.execute_input":"2025-03-30T21:03:20.612283Z","iopub.status.idle":"2025-03-30T21:47:43.748058Z","shell.execute_reply.started":"2025-03-30T21:03:20.612261Z","shell.execute_reply":"2025-03-30T21:47:43.747279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HeadCountingDataset(Dataset):\n    \"\"\"\n    Custom dataset that maps each image to its head count.\n    The head count is derived from the number of pseudo-label bounding boxes.\n    \"\"\"\n    def __init__(self, image_dir, pseudo_labels_path, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        with open(pseudo_labels_path, 'r') as f:\n            self.pseudo_labels = json.load(f)\n        self.image_files = list(self.pseudo_labels.keys())\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        image_file = self.image_files[idx]\n        image_path = os.path.join(self.image_dir, image_file)\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        # The head count is the number of bounding boxes detected.\n        count = len(self.pseudo_labels[image_file])\n        return image, torch.tensor([count], dtype=torch.float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:48:38.826998Z","iopub.execute_input":"2025-03-30T21:48:38.827318Z","iopub.status.idle":"2025-03-30T21:48:38.833282Z","shell.execute_reply.started":"2025-03-30T21:48:38.827296Z","shell.execute_reply":"2025-03-30T21:48:38.832460Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data Transformation","metadata":{}},{"cell_type":"code","source":"# image transformations (resize and normalize for pre-trained models)\ndata_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Loading","metadata":{}},{"cell_type":"code","source":"# Dataset directory and pseudo labels file (using Kaggle path)\nimage_dir = \"/kaggle/input/crowd-data/Train Data\"\npseudo_labels_path = \"/kaggle/working/pseudo_labels.json\"\n\n# Dataset instance\ndataset = HeadCountingDataset(image_dir, pseudo_labels_path, transform=data_transforms)\n\n# Split the dataset: 70% train, 10% validation, 20% test \ndataset_length = len(dataset)\ntrain_size = int(0.7 * dataset_length)\nval_size = int(0.1 * dataset_length)\ntest_size = dataset_length - train_size - val_size\n\nprint(f\"Total images: {dataset_length}\")\nprint(f\"Train size: {train_size}, Validation size: {val_size}, Test size: {test_size}\")\n\n# For training comparative models, we use train and validation sets.\ntrain_dataset, val_dataset,test_dataset  = random_split(dataset, [train_size, val_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\ndataloaders = {'train': train_loader, 'val': val_loader}\n\n# Create DataLoaders for each subset\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:48:44.011570Z","iopub.execute_input":"2025-03-30T21:48:44.011883Z","iopub.status.idle":"2025-03-30T21:48:44.029352Z","shell.execute_reply.started":"2025-03-30T21:48:44.011858Z","shell.execute_reply":"2025-03-30T21:48:44.028626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Transformation","metadata":{}},{"cell_type":"code","source":"def get_model(model_name='vgg16'):\n    \"\"\"\n    Loads a pre-trained CNN model and adapts it for regression (head count prediction).\n    \n    Args:\n        model_name (str): Choose 'vgg16' or 'resnet50'.\n    \n    Returns:\n        model: Modified pre-trained model.\n    \"\"\"\n    if model_name == 'vgg16':\n        model = models.vgg16(pretrained=True)\n        # Replace the last classifier layer for regression (output 1 value)\n        model.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n    elif model_name == 'resnet50':\n        model = models.resnet50(pretrained=True)\n        # Replace the final fully connected layer for regression\n        model.fc = nn.Linear(in_features=model.fc.in_features, out_features=1)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:48:56.560858Z","iopub.execute_input":"2025-03-30T21:48:56.561179Z","iopub.status.idle":"2025-03-30T21:48:56.565784Z","shell.execute_reply.started":"2025-03-30T21:48:56.561153Z","shell.execute_reply":"2025-03-30T21:48:56.564961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs, device):\n    \"\"\"\n    Train the model and record training and validation losses.\n    \n    Returns:\n        model: Best model based on validation loss.\n        train_losses: List of average training loss per epoch.\n        val_losses: List of average validation loss per epoch.\n    \"\"\"\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = float('inf')\n    \n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(\"-\" * 20)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n            \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            print(f\"{phase} Loss: {epoch_loss:.4f}\")\n            \n            if phase == 'train':\n                train_losses.append(epoch_loss)\n            else:\n                val_losses.append(epoch_loss)\n                scheduler.step(epoch_loss)\n            \n            if phase == 'val' and epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                \n        print()\n    \n    time_elapsed = time.time() - since\n    print(f\"Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n    print(f\"Best Validation Loss: {best_loss:.4f}\")\n    \n    model.load_state_dict(best_model_wts)\n    return model, train_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:48:58.590333Z","iopub.execute_input":"2025-03-30T21:48:58.590688Z","iopub.status.idle":"2025-03-30T21:48:58.598606Z","shell.execute_reply.started":"2025-03-30T21:48:58.590663Z","shell.execute_reply":"2025-03-30T21:48:58.597609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_epochs = 10\ncriterion = nn.MSELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:49:01.617034Z","iopub.execute_input":"2025-03-30T21:49:01.617319Z","iopub.status.idle":"2025-03-30T21:49:01.680780Z","shell.execute_reply.started":"2025-03-30T21:49:01.617298Z","shell.execute_reply":"2025-03-30T21:49:01.679822Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VGG16 Training","metadata":{}},{"cell_type":"code","source":"# ---- VGG16 Training ----\nprint(\"Training VGG16 Model\")\nvgg_model = get_model(\"vgg16\").to(device)\noptimizer_vgg = optim.Adam(vgg_model.parameters(), lr=1e-4)\nscheduler_vgg = optim.lr_scheduler.ReduceLROnPlateau(optimizer_vgg, mode='min', factor=0.1, patience=2)\nvgg_model, vgg_train_losses, vgg_val_losses = train_model(vgg_model, dataloaders, criterion, optimizer_vgg, scheduler_vgg, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T21:49:01.895954Z","iopub.execute_input":"2025-03-30T21:49:01.896243Z","iopub.status.idle":"2025-03-30T22:05:53.000491Z","shell.execute_reply.started":"2025-03-30T21:49:01.896221Z","shell.execute_reply":"2025-03-30T22:05:52.999517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ResNet-50 Training","metadata":{}},{"cell_type":"code","source":"# ---- ResNet50 Training ----\nprint(\"\\nTraining ResNet50 Model\")\nresnet_model = get_model(\"resnet50\").to(device)\noptimizer_resnet = optim.SGD(resnet_model.parameters(), lr=5e-4)\nscheduler_resnet = optim.lr_scheduler.ReduceLROnPlateau(optimizer_resnet, mode='min', factor=0.1, patience=2)\nresnet_model, resnet_train_losses, resnet_val_losses = train_model(resnet_model, dataloaders, criterion, optimizer_resnet, scheduler_resnet, num_epochs, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:13:48.118773Z","iopub.execute_input":"2025-03-30T22:13:48.119064Z","iopub.status.idle":"2025-03-30T22:30:30.679510Z","shell.execute_reply.started":"2025-03-30T22:13:48.119043Z","shell.execute_reply":"2025-03-30T22:30:30.678515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization of Loss Curves","metadata":{}},{"cell_type":"markdown","source":"#### VGG16","metadata":{}},{"cell_type":"code","source":"epochs = range(1, num_epochs + 1)\n\nplt.figure(figsize=(12, 5))\n\n# VGG16 Loss Curves\nplt.subplot(1, 2, 1)\nplt.plot(epochs, vgg_train_losses, 'b-o', label='VGG16 Train Loss')\nplt.plot(epochs, vgg_val_losses, 'r-o', label='VGG16 Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('VGG16 Loss Curves')\nplt.legend()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:33:59.902317Z","iopub.execute_input":"2025-03-30T22:33:59.902753Z","iopub.status.idle":"2025-03-30T22:34:00.121604Z","shell.execute_reply.started":"2025-03-30T22:33:59.902720Z","shell.execute_reply":"2025-03-30T22:34:00.120683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Resnet50","metadata":{}},{"cell_type":"code","source":"# ResNet50 Loss Curves\nplt.subplot(1, 2, 2)\nplt.plot(epochs, resnet_train_losses, 'b-o', label='ResNet50 Train Loss')\nplt.plot(epochs, resnet_val_losses, 'r-o', label='ResNet50 Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('ResNet50 Loss Curves')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:34:03.362897Z","iopub.execute_input":"2025-03-30T22:34:03.363231Z","iopub.status.idle":"2025-03-30T22:34:03.556784Z","shell.execute_reply.started":"2025-03-30T22:34:03.363202Z","shell.execute_reply":"2025-03-30T22:34:03.556049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Descriptive Statistics about our Image Data","metadata":{}},{"cell_type":"code","source":"# Load pseudo-labels from JSON\nwith open(pseudo_labels_path, 'r') as f:\n    pseudo_labels = json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a list to store extracted features for each image\nfeatures_list = []\n\n# Loop over each image and its corresponding pseudo-labels\nfor image_file, boxes in pseudo_labels.items():\n    image_path = os.path.join(image_dir, image_file)\n    try:\n        # Open the image and convert to RGB\n        img = Image.open(image_path).convert('RGB')\n    except Exception as e:\n        print(f\"Error processing {image_file}: {e}\")\n        continue\n    \n    # Feature Extraction \n    # 1. Head Count: number of pseudo-label bounding boxes\n    head_count = len(boxes)\n    \n    # 2. Image Dimensions: width and height (in pixels)\n    width, height = img.size\n    \n    # 3. Area: total number of pixels (width x height)\n    area = width * height\n    \n    # 4. Aspect Ratio: width divided by height\n    aspect_ratio = width / height if height != 0 else np.nan\n    \n    # 5. Average Brightness: mean pixel value from grayscale conversion\n    img_gray = img.convert('L')\n    avg_brightness = np.mean(np.array(img_gray))\n    \n    # 6. File Size: size of the image file in kilobytes\n    try:\n        file_size = os.path.getsize(image_path) / 1024  # in KB\n    except Exception as e:\n        file_size = np.nan\n\n    # Append all features into the list as a dictionary\n    features_list.append({\n        'image_file': image_file,\n        'head_count': head_count,\n        'width': width,\n        'height': height,\n        'area': area,\n        'aspect_ratio': aspect_ratio,\n        'avg_brightness': avg_brightness,\n        'file_size_kb': file_size\n    })\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DataFrame from the extracted features\ndf = pd.DataFrame(features_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T22:34:14.514404Z","iopub.execute_input":"2025-03-30T22:34:14.514727Z","iopub.status.idle":"2025-03-30T22:34:14.938535Z","shell.execute_reply.started":"2025-03-30T22:34:14.514703Z","shell.execute_reply":"2025-03-30T22:34:14.937855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Descriptive Statistical Analysis","metadata":{}},{"cell_type":"code","source":"# Compute overall descriptive statistics for numerical features\ndf.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:20:51.444120Z","iopub.execute_input":"2025-03-31T00:20:51.444506Z","iopub.status.idle":"2025-03-31T00:20:51.475749Z","shell.execute_reply.started":"2025-03-31T00:20:51.444477Z","shell.execute_reply":"2025-03-31T00:20:51.474896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute and print the mode for each numeric feature\nnumeric_features = ['head_count', 'width', 'height', 'area', 'aspect_ratio', 'avg_brightness', 'file_size_kb']\nprint(\"\\nMode for each numeric feature:\")\nfor feature in numeric_features:\n    mode_val = df[feature].mode()\n    print(f\"{feature} mode: {mode_val.values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Graphical Representation","metadata":{}},{"cell_type":"code","source":"# 3.1 Histograms for Each Feature\nplt.figure(figsize=(16, 12))\nfor i, feature in enumerate(numeric_features, 1):\n    plt.subplot(3, 3, i)\n    plt.hist(df[feature].dropna(), bins=10, color='skyblue', edgecolor='black')\n    plt.title(f\"Histogram of {feature}\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:23:59.118806Z","iopub.execute_input":"2025-03-31T00:23:59.119112Z","iopub.status.idle":"2025-03-31T00:24:00.427801Z","shell.execute_reply.started":"2025-03-31T00:23:59.119093Z","shell.execute_reply":"2025-03-31T00:24:00.427031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3.2 Line Plot: Head Count Sorted by Image Index\ndf_sorted = df.sort_values(by='head_count').reset_index(drop=True)\nplt.figure(figsize=(10, 5))\nplt.plot(df_sorted.index, df_sorted['head_count'], marker='o', linestyle='-')\nplt.title(\"Head Count Sorted by Image Index\")\nplt.xlabel(\"Sorted Image Index\")\nplt.ylabel(\"Head Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:24:00.429168Z","iopub.execute_input":"2025-03-31T00:24:00.429454Z","iopub.status.idle":"2025-03-31T00:24:00.600439Z","shell.execute_reply.started":"2025-03-31T00:24:00.429430Z","shell.execute_reply":"2025-03-31T00:24:00.599541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3.3 Bar Plot: Head Count for Each Image\nplt.figure(figsize=(12, 6))\nplt.bar(df['image_file'], df['head_count'], color='lightgreen')\nplt.xticks(rotation=90)\nplt.title(\"Head Count for Each Image\")\nplt.xlabel(\"Image File\")\nplt.ylabel(\"Head Count\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:24:04.567634Z","iopub.execute_input":"2025-03-31T00:24:04.567939Z","iopub.status.idle":"2025-03-31T00:24:05.714345Z","shell.execute_reply.started":"2025-03-31T00:24:04.567916Z","shell.execute_reply":"2025-03-31T00:24:05.713399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3.4 Correlation Analysis: Correlation Matrix and Heatmap\ncorr_matrix = df[numeric_features].corr()\nprint(\"\\nCorrelation Matrix:\")\ncorr_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:24:15.324967Z","iopub.execute_input":"2025-03-31T00:24:15.325267Z","iopub.status.idle":"2025-03-31T00:24:15.340079Z","shell.execute_reply.started":"2025-03-31T00:24:15.325244Z","shell.execute_reply":"2025-03-31T00:24:15.339181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.imshow(corr_matrix, cmap='viridis', interpolation='none')\nplt.colorbar()\nplt.xticks(range(len(numeric_features)), numeric_features, rotation=90)\nplt.yticks(range(len(numeric_features)), numeric_features)\nplt.title(\"Correlation Matrix Heatmap\")\nplt.show()\n\n# 3.5 Scatter Plots: Head Count vs. Each Other Feature\nfor feature in numeric_features:\n    if feature != 'head_count':\n        plt.figure(figsize=(6, 4))\n        plt.scatter(df[feature], df['head_count'], alpha=0.7, color='coral')\n        plt.xlabel(feature)\n        plt.ylabel(\"Head Count\")\n        plt.title(f\"Head Count vs. {feature}\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:24:22.945013Z","iopub.execute_input":"2025-03-31T00:24:22.945340Z","iopub.status.idle":"2025-03-31T00:24:24.118606Z","shell.execute_reply.started":"2025-03-31T00:24:22.945313Z","shell.execute_reply":"2025-03-31T00:24:24.117588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3.6 Distance Matrix: Compute and Visualize the Euclidean Distance Matrix\ndistance_matrix = squareform(pdist(df[numeric_features], metric='euclidean'))\nprint(\"\\nDistance Matrix (first 5 rows and columns):\")\nprint(distance_matrix[:5, :5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:22:15.188736Z","iopub.execute_input":"2025-03-31T00:22:15.189078Z","iopub.status.idle":"2025-03-31T00:22:15.196364Z","shell.execute_reply.started":"2025-03-31T00:22:15.189051Z","shell.execute_reply":"2025-03-31T00:22:15.195581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sorting the data by head count (descending order)\ndf_sorted_by_head = df.sort_values(by='head_count', ascending=False)\nprint(\"\\nData sorted by head_count (top 5):\")\ndf_sorted_by_head.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:22:44.199156Z","iopub.execute_input":"2025-03-31T00:22:44.199491Z","iopub.status.idle":"2025-03-31T00:22:44.211335Z","shell.execute_reply.started":"2025-03-31T00:22:44.199465Z","shell.execute_reply":"2025-03-31T00:22:44.210477Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Testing on Augmented Dataset","metadata":{}},{"cell_type":"code","source":"# 1. Create a random subset (30% of the images)\ndataset_length = len(dataset)\nsubset_size = int(0.3 * dataset_length)\nsubset_indices = random.sample(range(dataset_length), subset_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Define augmentation transformations to simulate varying conditions\nclass EnsureTensor(object):\n    def __call__(self, img):\n        if not isinstance(img, torch.Tensor):\n            return transforms.ToTensor()(img)\n        return img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transformations","metadata":{}},{"cell_type":"code","source":"augmented_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n    transforms.RandomRotation(degrees=15),\n    EnsureTensor(),  # This ensures the image is a tensor\n    transforms.RandomErasing(p=0.5, scale=(0.02, 0.15)),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Augmented Image Result","metadata":{}},{"cell_type":"code","source":"random_index = random.randint(0, len(augmented_dataset) - 1)\nimg_tensor, label = augmented_dataset[random_index]\n\n# Function to denormalize the image tensor\ndef denormalize(tensor, mean, std):\n    # Cloning the tensor to avoid modifying the original image\n    tensor = tensor.clone()\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m)\n    return tensor\n\n# Mean and std used for normalization in the transform pipeline\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Denormalize the image tensor\nimg_denorm = denormalize(img_tensor, mean, std)\n\n# Convert the tensor to a NumPy array and change the shape from (C, H, W) to (H, W, C)\nimg_np = img_denorm.numpy().transpose((1, 2, 0))\nimg_np = np.clip(img_np, 0, 1)  # Ensure the pixel values are in [0, 1]\n\n# Display the image\nplt.imshow(img_np)\nplt.title(\"Augmented Image\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:03:57.065219Z","iopub.execute_input":"2025-03-31T00:03:57.065637Z","iopub.status.idle":"2025-03-31T00:03:58.335885Z","shell.execute_reply.started":"2025-03-31T00:03:57.065608Z","shell.execute_reply":"2025-03-31T00:03:58.335108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating the augmented dataset instance and selecting the subset\naugmented_dataset = HeadCountingDataset(image_dir, pseudo_labels_path, transform=augmented_transforms)\naugmented_subset = Subset(augmented_dataset, subset_indices)\naugmented_loader = DataLoader(augmented_subset, batch_size=16, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation function to compute MSE and MAE","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()\n    total_loss = 0.0\n    total_mae = 0.0\n    total_samples = 0\n    criterion = nn.MSELoss()\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * images.size(0)\n            mae = torch.abs(outputs - labels).mean().item() * images.size(0)\n            total_mae += mae\n            total_samples += images.size(0)\n    avg_loss = total_loss / total_samples\n    avg_mae = total_mae / total_samples\n    return avg_loss, avg_mae","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Evaluate both models on the augmented subset","metadata":{}},{"cell_type":"code","source":"resnet_mse, resnet_mae = evaluate_model(resnet_model, augmented_loader, device)\nvgg_mse, vgg_mae = evaluate_model(vgg_model, augmented_loader, device)\n\nprint(f\"ResNet50 on Augmented Data - MSE: {resnet_mse:.4f}, MAE: {resnet_mae:.4f}\")\nprint(f\"VGG16 on Augmented Data - MSE: {vgg_mse:.4f}, MAE: {vgg_mae:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T23:59:32.159639Z","iopub.execute_input":"2025-03-30T23:59:32.159947Z","iopub.status.idle":"2025-03-31T00:01:01.034834Z","shell.execute_reply.started":"2025-03-30T23:59:32.159923Z","shell.execute_reply":"2025-03-31T00:01:01.034006Z"}},"outputs":[],"execution_count":null}]}